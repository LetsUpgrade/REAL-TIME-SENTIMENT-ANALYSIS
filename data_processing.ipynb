{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "ps = PorterStemmer()\n",
    "ls = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211746, 12)\n",
      "0         How has #PTSD affected this writer's #quaranti...\n",
      "1          #blackandyellow #Love #moments #september #Au...\n",
      "2         Things I Do In My Spare Time Play Game 1- Eco ...\n",
      "3         每天花5-30分钟进行正念冥想。使用此链接可帮助您开始冥想：https://www.apa....\n",
      "4         Your #Quarantine #MovieList: THE HOUSE ON SORO...\n",
      "                                ...                        \n",
      "211741    A man in Gujranwala* got his ass kicked by his...\n",
      "211742    @joobsie Astaga! Dua kali aku kena slap malam ...\n",
      "211743    RT @sfrizwan: #ViolenceAgainstMen RT @AmazingS...\n",
      "211744    #ViolenceAgainstMen RT @AmazingSusan Women rap...\n",
      "211745    Spotlight On Male Victims Of Domestic Abuse ht...\n",
      "Name: text, Length: 211746, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Dataset.csv\")              \n",
    "print(df.shape)\n",
    "tweets = df['text']\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [how, has, affected, this, writer, is, experie...\n",
       "1                                                        []\n",
       "2         [things, i, do, in, my, spare, time, play, gam...\n",
       "3                                                        []\n",
       "4         [your, the, house, on, sorority, row, my, review]\n",
       "                                ...                        \n",
       "211741    [a, man, in, gujranwala, got, his, ass, kicked...\n",
       "211742    [astaga, dua, kali, aku, kena, slap, malam, ni...\n",
       "211743                       [rt, rt, women, rape, men, in]\n",
       "211744                           [rt, women, rape, men, in]\n",
       "211745    [spotlight, on, male, victims, of, domestic, a...\n",
       "Name: text, Length: 211746, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def cleaning(text):\n",
    "    text = decontracted(text)\n",
    "    text = text.lower()                              #lowering the text\n",
    "    text = re.sub(r'@\\S+','',text)                   #Removed @mentions\n",
    "    text = re.sub(r'#\\S+','',text)                   #Remove the hyper link\n",
    "    text = re.sub(r'RT\\S+','',text)                  #Removing ReTweets\n",
    "    text = re.sub(r'https?\\S+','',text)              #Remove the hyper link\n",
    "    text = re.sub('[^a-z]',' ',text)              #Remove the character other than alphabet\n",
    "    text = text.split()\n",
    "    return text\n",
    "tweets_cleaned =  tweets.apply(cleaning)\n",
    "tweets_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tweets_stemmed = tweets_cleaned.apply(lambda x: [ps.stem(word) for word in x if word not in stop_words])\n",
    "tweets_lemmatized = tweets_cleaned.apply(lambda x: [ls.lemmatize(word) for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_stemmed = tweets_stemmed.apply(lambda x: ' '.join(x))\n",
    "tweets_lemmatized = tweets_lemmatized.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['scrapped_text'] = df['text']\n",
    "df['Lemmatized_text'] = tweets_lemmatized.to_frame() \n",
    "df['Stemmed_text'] = tweets_lemmatized.to_frame()\n",
    "new_df=df.drop(['Unnamed: 0', 'id', 'permalink', 'username', 'to', 'text', 'date','retweets', 'favorites', 'mentions', 'hashtags', 'geo',],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scrapped_text</th>\n",
       "      <th>Lemmatized_text</th>\n",
       "      <th>Stemmed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How has #PTSD affected this writer's #quaranti...</td>\n",
       "      <td>affected writer experience click link find</td>\n",
       "      <td>affected writer experience click link find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#blackandyellow #Love #moments #september #Au...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Things I Do In My Spare Time Play Game 1- Eco ...</td>\n",
       "      <td>thing spare time play game eco unisex tee usd ...</td>\n",
       "      <td>thing spare time play game eco unisex tee usd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>每天花5-30分钟进行正念冥想。使用此链接可帮助您开始冥想：https://www.apa....</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Your #Quarantine #MovieList: THE HOUSE ON SORO...</td>\n",
       "      <td>house sorority row review</td>\n",
       "      <td>house sorority row review</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       scrapped_text  \\\n",
       "0  How has #PTSD affected this writer's #quaranti...   \n",
       "1   #blackandyellow #Love #moments #september #Au...   \n",
       "2  Things I Do In My Spare Time Play Game 1- Eco ...   \n",
       "3  每天花5-30分钟进行正念冥想。使用此链接可帮助您开始冥想：https://www.apa....   \n",
       "4  Your #Quarantine #MovieList: THE HOUSE ON SORO...   \n",
       "\n",
       "                                     Lemmatized_text  \\\n",
       "0         affected writer experience click link find   \n",
       "1                                                      \n",
       "2  thing spare time play game eco unisex tee usd ...   \n",
       "3                                                      \n",
       "4                          house sorority row review   \n",
       "\n",
       "                                        Stemmed_text  \n",
       "0         affected writer experience click link find  \n",
       "1                                                     \n",
       "2  thing spare time play game eco unisex tee usd ...  \n",
       "3                                                     \n",
       "4                          house sorority row review  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('New_Dataset_3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
